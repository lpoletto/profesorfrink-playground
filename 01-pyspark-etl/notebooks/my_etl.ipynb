{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a13506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ as env\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4b844c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.4.0\n",
      "Spark App Name: My ETL\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName(\"My ETL\") \\\n",
    "                    .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:12.4.2.jre11\") \\\n",
    "                    .config(\"spark.jars\", env[\"SPARK_JAR_PATH\"]) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Spark Version: \"+ spark.version)\n",
    "print(\"Spark App Name: \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abedc68f",
   "metadata": {},
   "source": [
    "Read SQL Server Table to PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fb9e2",
   "metadata": {},
   "source": [
    "Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ecaeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(tbl_name, schema=\"dbo\", output_path=\"/tmp\"):\n",
    "    \n",
    "    server_name = env[\"MSSQL_HOST\"]\n",
    "    database_name = env[\"MSSQL_DB\"]\n",
    "    username = env[\"MSSQL_SA_USER\"]\n",
    "    password = env[\"MSSQL_SA_PASSWORD\"]\n",
    "    port = env[\"MSSQL_PORT\"]\n",
    "    jdbc_url = f\"jdbc:sqlserver://{server_name}:{port};database={database_name};user={username};password={password};trustServerCertificate=true;\"\n",
    "\n",
    "    try:\n",
    "        sql_query = f\"\"\"SELECT * FROM {schema}.{tbl_name}\"\"\"\n",
    "        \n",
    "        df_src = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"query\", sql_query) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "\n",
    "        # print(f\"Total rows: {df_src.count()}\")\n",
    "        # df_src.show(5, truncate=False)\n",
    "        # df_src.printSchema()\n",
    "\n",
    "        final_output_path= f\"{output_path}/{tbl_name}\"\n",
    "\n",
    "        df_src.write.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(final_output_path)\n",
    "\n",
    "        print(f\"Datos cargados existosamente en {final_output_path}\")\n",
    "        return final_output_path\n",
    "    except Exception as e:\n",
    "        print(\"Error al extraer datos: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e496929",
   "metadata": {},
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f644276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(input_path):\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .load(input_path) \\\n",
    "            .select(\"CustomerKey\" \n",
    "                ,\"CustomerAlternateKey\" \n",
    "                ,\"FirstName\" \n",
    "                ,\"LastName\" \n",
    "                ,\"BirthDate\" \n",
    "                ,\"EmailAddress\"\n",
    "            )\n",
    "\n",
    "        tranformed_df = df.withColumn(\"FullName\", concat_ws(\" \", df.FirstName, df.LastName)) \\\n",
    "            .withColumn(\"DBSource\", lit(\"AdventureWorks\")) \\\n",
    "            .withColumn(\"IngestionDate\", current_date())\n",
    "\n",
    "        tranformed_df = tranformed_df.select(\"CustomerKey\" \n",
    "                ,\"CustomerAlternateKey\" \n",
    "                ,\"FirstName\" \n",
    "                ,\"LastName\"\n",
    "                ,\"FullName\" \n",
    "                ,\"BirthDate\" \n",
    "                ,\"EmailAddress\"\n",
    "                ,\"DBSource\"\n",
    "                ,\"IngestionDate\")\n",
    "        \n",
    "        return tranformed_df\n",
    "    except Exception as e:\n",
    "        print(\"Error al transformar los datos: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338390ab",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be55765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, tbl_name):\n",
    "    try:\n",
    "        jdbc_url = f\"jdbc:postgresql://{env['POSTGRES_HOST']}:{env['POSTGRES_PORT']}/{env['POSTGRES_DB']}?user={env['POSTGRES_USER']}&password={env['POSTGRES_PASSWORD']}\"\n",
    "\n",
    "        df.write.mode(\"overwrite\") \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", jdbc_url) \\\n",
    "                    .option(\"dbtable\", \"stg_\"+tbl_name) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .save()\n",
    "        print(f\"Datos cargados existosamente en la tabla stg_{tbl_name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al cargar los datos: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0da5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl(src_table, tgt_table):\n",
    "    input_path=extract(src_table)\n",
    "\n",
    "    df = transform(input_path)\n",
    "\n",
    "    tbl_name = tgt_table\n",
    "\n",
    "    load(df, tbl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd626a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados existosamente en /tmp/DimCustomer\n"
     ]
    }
   ],
   "source": [
    "run_etl(\"DimCustomer\", \"dim_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01aba90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
